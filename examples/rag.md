# RAG & Ingestion ğŸ“š

This guide explains how to configure an ingestion pipeline for your documents and how to extend the system to support new file formats (like PDFs) by defining custom Readers.

## Ingestion Pipeline âš™ï¸

The `IngestionPipeline` class is the heart of the RAG system. It automatically handles:
1.  Reading files from the specified directory.
2.  Splitting text into chunks (Chunking).
3.  Generating embeddings.
4.  Saving to the Vector DB (ChromaDB).
5.  (Optional) Generating additional context to improve retrieval (`contextual_retrieval`).

Here is how to configure and run a pipeline to ingest Markdown files:

```python
import os
from typing import List
from lite_agents.llm import LiteLLM
from lite_agents.db import ChromaDB
from lite_agents.ingestion import LiteIngestion
# 1. Embedding Configuration ğŸ§ 
# Define the function that generates embeddings (e.g., using OpenAI, Azure, or local models)
def create_embeddings(texts: List[str]) -> List[List[float]]:
    # Dummy example. Replace with the actual call to the provider (e.g., OpenAI)
    # return client.embeddings.create(input=texts, model="text-embedding-3-small").data
    pass

# 2. Component Initialization ğŸ—ï¸
# LLM used to generate context (if use_contextual_retrieval=True)
llm = LiteLLM(model="gpt-4o-mini", temperature=0)

# Vector Database to save chunks
db = ChromaDB(
    persist_path="./chroma_db",
    collection_name="company_knowledge"
)

# 3. Pipeline Creation ğŸš€
ingestion = LiteIngestion(
    llm=llm,
    vector_db=db,
    embedding_function=create_embeddings,
    chunk_size=800,
    chunk_overlap=200,
    add_context=True
)

# 4. Execution â–¶ï¸
# Process documents to get chunks
chunks = ingestion.process_directory(
    directory="./company_policies",
    file_pattern="*.md"
)

# Ingest chunks into the database
ingestion.ingest_chunks(chunks)
```

## Custom Readers (PDF Support) ğŸ§©

By default, `lite-agents` supports Markdown files. However, the system is extensible: you can add support for any file format (PDF, DOCX, TXT) by registering a new Reader at runtime.

You don't need to modify the library code. Just define a class and decorate it with `@register_reader`.

### Example: PDF Reader ğŸ“„

Here is how to implement a PDF reader and make it immediately available to the `LiteIngestion` pipeline.

```python
from typing import List, Tuple
from pathlib import Path
from lite_agents.readers import BaseReader, register_reader

# 1. Use the decorator to associate the .pdf extension with this class
@register_reader([".pdf"])
class PDFReader(BaseReader):
    
    def read(self, file_path: Path) -> Tuple[str, str, str]:
        """
        Reads the PDF file and extracts its content.
        
        Args:
            file_path: The path of the file to read.
            
        Returns:
            A tuple containing: (file_name, document_title, text_content)
        """
        # Here you can use libraries like pypdf, pdfplumber, or PyMuPDF
        # Pseudocode example:
        # text = pdf_library.extract_text(file_path)
        
        print(f"Reading PDF: {file_path.name}")
        extracted_text = "Content extracted from PDF..." # Replace with actual logic
        
        # Return the file name, a title (or file stem), and the text
        return file_path.name, file_path.stem, extracted_text

    def split(self, content: str) -> List[Tuple[str, str]]:
        """
        (Optional) Splits the raw content into logical sections before chunking.
        If not implemented, chunking will happen on the entire text.
        
        Returns:
            A list of tuples: (section_title, section_content)
        """
        # Simple example: returns the entire content as a single section
        return [("Full Document", content)]

# Done! ğŸ‰
# Now, when you run LiteIngestion, .pdf files in the input directory
# will be automatically processed using this class.
```

## Tips ğŸ’¡

### Exploring Generated Chunks ğŸ”

You can inspect the chunks generated by `LiteIngestion` to verify the process.

```python
import json

# Get statistics about processed documents and chunks
stats = ingestion.get_statistics(chunks)
print(json.dumps(stats, indent=2))

# Save chunks to a JSON file for inspection (no embeddings)
ingestion.save_chunks_to_json(chunks, "ingested_chunks.json")
```

### Testing Retrieval ğŸ”

You can test the retrieval quality by running a query against the database.

```python
query = "How can I request time off?"

# 1. Generate embedding for the query
# Note: create_embeddings returns a list of lists, so we take the first element
query_embedding = create_embeddings([query])[0]

# 2. Query the database
results = db.query(query_embeddings=query_embedding, n_results=3)

# 3. Print results
for i, res in enumerate(results):
    print(f"Result #{i+1} (Distance: {res['distance']:.4f})")
    print(f"Content: {res['content'][:100]}...")
    print(f"Metadata: {res['metadata']}")
    print("-" * 40)
```
